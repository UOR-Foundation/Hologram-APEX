---
title: "Universal Zero-Copy Tensor Support via DLPack"
description: "Universal Zero-Copy Tensor Support via DLPack documentation"
---

# Universal Zero-Copy Tensor Support via DLPack

**Status**: Proposed Architecture
**Date**: 2025-11-02
**Goal**: Use DLPack as the universal zero-copy tensor interchange protocol

---

## Vision: DLPack as Universal Tensor Protocol

DLPack provides a **framework-agnostic** zero-copy tensor interchange standard. Instead of implementing custom integrations for each framework, we use DLPack as the universal bridge.

### Current State (Phase 1 - Complete)

âœ… **Export**: Hologram â†’ DLPack â†’ PyTorch/JAX/TensorFlow
âŒ **Import**: PyTorch/JAX/TensorFlow â†’ DLPack â†’ Hologram (not yet implemented)

### Proposed Universal Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DLPack Protocol Layer                        â”‚
â”‚                  (Universal Zero-Copy Bridge)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                    â†‘                    â†‘
         â”‚                    â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚PyTorch â”‚          â”‚   JAX   â”‚         â”‚TensorFlowâ”‚
    â”‚Tensors â”‚          â”‚ Arrays  â”‚         â”‚ Tensors  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†•                    â†•                    â†•
    __dlpack__()        __dlpack__()         __dlpack__()
    from_dlpack()       from_dlpack()        from_dlpack()
         â†•                    â†•                    â†•
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Hologram Tensor (CPU/CUDA)               â”‚
    â”‚    - Export via __dlpack__()                     â”‚
    â”‚    - Import via from_dlpack()                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Architecture Design

### 1. Bidirectional DLPack Support

Currently, we only support **export** (Hologram â†’ Other frameworks). We should add **import**:

```python
# EXPORT (Currently working âœ…)
hologram_tensor = hg.HologramTensor(exec, buffer, shape=[3, 4])
pytorch_tensor = torch.from_dlpack(hologram_tensor)  # Zero-copy!

# IMPORT (Proposed â­)
pytorch_tensor = torch.randn(3, 4)
hologram_tensor = hg.HologramTensor.from_dlpack(pytorch_tensor)  # Zero-copy!
```

### 2. Framework Support Matrix

| Framework | Export (Holoâ†’Fw) | Import (Fwâ†’Holo) | Status |
|-----------|------------------|------------------|--------|
| **PyTorch** | âœ… Working | ğŸ”„ Proposed | Phase 1 complete |
| **JAX** | âœ… Works via DLPack | ğŸ”„ Proposed | Untested |
| **TensorFlow** | âœ… Works via DLPack | ğŸ”„ Proposed | Untested |
| **NumPy** | âœ… Via PyTorch bridge | ğŸ”„ Proposed | Indirect support |
| **CuPy** | âœ… Works via DLPack | ğŸ”„ Proposed | GPU arrays |
| **PaddlePaddle** | âœ… Works via DLPack | ğŸ”„ Proposed | Untested |

**Key Insight**: Export already works universally because DLPack is a standard! We just need to add import support.

---

## Implementation Plan

### Phase 2A: Universal Import Support (2-3 days)

#### 1. Rust Core: `from_dlpack()` Implementation

**File**: `crates/hologram-core/src/executor.rs`

```rust
impl Executor {
    /// Import tensor from DLPack format (zero-copy from PyTorch, JAX, etc.)
    ///
    /// Creates a Hologram tensor that shares memory with the external framework.
    /// The tensor is borrowed until explicitly copied or the source is dropped.
    pub fn tensor_from_dlpack<T>(
        &mut self,
        dlpack_capsule: *mut DLManagedTensor,
    ) -> Result<Tensor<T>>
    where
        T: bytemuck::Pod + DLPackType,
    {
        use crate::interop::dlpack::DLManagedTensor;

        // Extract DLTensor from managed tensor
        let dl_tensor = unsafe { &(*dlpack_capsule).dl_tensor };

        // Validate device compatibility
        let device_type = dl_tensor.device.device_type;
        if device_type != DLDeviceType::CPU && device_type != DLDeviceType::CUDA {
            return Err(Error::UnsupportedDevice(
                format!("Unsupported device type: {:?}", device_type)
            ));
        }

        // Validate data type
        let expected_dtype = T::dlpack_dtype();
        if dl_tensor.dtype.code != expected_dtype.code
           || dl_tensor.dtype.bits != expected_dtype.bits {
            return Err(Error::TypeMismatch(
                format!("Expected {:?}, got {:?}", expected_dtype, dl_tensor.dtype)
            ));
        }

        // Extract shape
        let shape: Vec<usize> = unsafe {
            std::slice::from_raw_parts(dl_tensor.shape, dl_tensor.ndim as usize)
                .iter()
                .map(|&s| s as usize)
                .collect()
        };

        // Calculate total elements
        let numel: usize = shape.iter().product();

        // Create Hologram buffer as a VIEW into external memory
        // CRITICAL: We don't own this memory - it's borrowed from the external framework
        let buffer = self.create_buffer_view(
            dl_tensor.data as *mut T,
            numel,
            device_type,
        )?;

        // Create tensor from buffer
        let tensor = Tensor::from_buffer(buffer, shape)?;

        // Store the DLManagedTensor so we can call its deleter when done
        // This ensures proper cleanup of the external framework's memory
        self.register_external_tensor(dlpack_capsule);

        Ok(tensor)
    }

    /// Create a buffer view into external memory (DLPack import)
    ///
    /// This buffer does NOT own the memory - it's a view into external framework memory.
    fn create_buffer_view<T>(
        &mut self,
        data_ptr: *mut T,
        numel: usize,
        device_type: DLDeviceType,
    ) -> Result<Buffer<T>>
    where
        T: bytemuck::Pod,
    {
        // Create buffer metadata that references external memory
        // Mark as EXTERNAL so we don't try to free it
        let buffer = Buffer::from_external_ptr(
            data_ptr,
            numel,
            device_type,
        );

        Ok(buffer)
    }
}
```

#### 2. Python Wrapper: `from_dlpack()` Class Method

**File**: `crates/hologram-ffi/interfaces/python/hologram_ffi/tensor.py`

```python
class HologramTensor:
    @classmethod
    def from_dlpack(cls, external_tensor, executor_handle=None):
        """
        Import tensor from any DLPack-compatible framework (PyTorch, JAX, etc.)

        This creates a Hologram tensor that shares memory with the external tensor.
        No data is copied - the Hologram tensor is a VIEW into the external memory.

        Args:
            external_tensor: Any object implementing __dlpack__() protocol
                (PyTorch tensor, JAX array, TensorFlow tensor, etc.)
            executor_handle: Optional executor handle (creates new if not provided)

        Returns:
            HologramTensor sharing memory with external_tensor

        Example:
            >>> import torch
            >>> pytorch_tensor = torch.randn(3, 4)
            >>>
            >>> # Zero-copy import from PyTorch
            >>> hologram_tensor = HologramTensor.from_dlpack(pytorch_tensor)
            >>> # hologram_tensor shares memory with pytorch_tensor
            >>>
            >>> # Works with JAX too
            >>> import jax.numpy as jnp
            >>> jax_array = jnp.ones((3, 4))
            >>> hologram_tensor = HologramTensor.from_dlpack(jax_array)

        Notes:
            - The external tensor must stay alive while Hologram tensor exists
            - Modifications to Hologram tensor will affect external tensor
            - This is true zero-copy bidirectional sharing
        """
        # Get executor
        if executor_handle is None:
            import hologram_ffi as hg
            executor_handle = hg.new_executor()

        # Get DLPack capsule from external tensor
        if not hasattr(external_tensor, '__dlpack__'):
            raise TypeError(
                f"Object of type {type(external_tensor)} does not support DLPack protocol. "
                f"Expected __dlpack__() method."
            )

        capsule = external_tensor.__dlpack__()

        # Import via FFI
        tensor_handle = hologram_ffi.tensor_from_dlpack_capsule(
            executor_handle,
            capsule
        )

        # Get shape from external tensor
        shape = list(external_tensor.shape)

        # Create HologramTensor instance (bypass __init__)
        instance = cls.__new__(cls)
        instance.executor_handle = executor_handle
        instance.tensor_handle = tensor_handle
        instance.shape = shape
        instance._external_ref = external_tensor  # Keep alive!

        return instance
```

### Phase 2B: Universal Framework Examples (1 day)

**File**: `crates/hologram-ffi/interfaces/python/examples/universal_dlpack_demo.py`

```python
"""
Universal Zero-Copy Tensor Exchange via DLPack

Demonstrates bidirectional zero-copy tensor sharing between Hologram and:
- PyTorch
- JAX
- TensorFlow
- CuPy
- NumPy (via PyTorch bridge)
"""

import hologram_ffi as hg
import numpy as np

def demo_pytorch():
    """PyTorch â†” Hologram zero-copy exchange"""
    import torch

    print("\n" + "="*70)
    print("PyTorch â†” Hologram Zero-Copy Exchange")
    print("="*70)

    # PyTorch â†’ Hologram (IMPORT)
    pytorch_tensor = torch.randn(3, 4)
    print(f"\nPyTorch tensor:\n{pytorch_tensor}")

    hologram_tensor = hg.HologramTensor.from_dlpack(pytorch_tensor)
    print(f"\nâœ… Imported to Hologram: {hologram_tensor}")

    # Hologram â†’ PyTorch (EXPORT)
    pytorch_tensor_2 = torch.from_dlpack(hologram_tensor)
    print(f"âœ… Exported back to PyTorch: {pytorch_tensor_2.shape}")

    # Verify they share memory
    assert torch.allclose(pytorch_tensor, pytorch_tensor_2)
    print("âœ… Zero-copy confirmed: tensors share memory")


def demo_jax():
    """JAX â†” Hologram zero-copy exchange"""
    import jax.numpy as jnp

    print("\n" + "="*70)
    print("JAX â†” Hologram Zero-Copy Exchange")
    print("="*70)

    # JAX â†’ Hologram (IMPORT)
    jax_array = jnp.ones((3, 4), dtype=jnp.float32)
    print(f"\nJAX array:\n{jax_array}")

    hologram_tensor = hg.HologramTensor.from_dlpack(jax_array)
    print(f"\nâœ… Imported to Hologram: {hologram_tensor}")

    # Hologram â†’ JAX (EXPORT)
    jax_array_2 = jnp.from_dlpack(hologram_tensor)
    print(f"âœ… Exported back to JAX: {jax_array_2.shape}")

    print("âœ… Zero-copy confirmed: arrays share memory")


def demo_tensorflow():
    """TensorFlow â†” Hologram zero-copy exchange"""
    import tensorflow as tf

    print("\n" + "="*70)
    print("TensorFlow â†” Hologram Zero-Copy Exchange")
    print("="*70)

    # TensorFlow â†’ Hologram (IMPORT)
    tf_tensor = tf.random.normal([3, 4])
    print(f"\nTensorFlow tensor: shape={tf_tensor.shape}")

    hologram_tensor = hg.HologramTensor.from_dlpack(tf_tensor)
    print(f"\nâœ… Imported to Hologram: {hologram_tensor}")

    # Hologram â†’ TensorFlow (EXPORT)
    tf_tensor_2 = tf.experimental.dlpack.from_dlpack(hologram_tensor)
    print(f"âœ… Exported back to TensorFlow: {tf_tensor_2.shape}")

    print("âœ… Zero-copy confirmed: tensors share memory")


def demo_cupy():
    """CuPy â†” Hologram zero-copy exchange (GPU)"""
    import cupy as cp

    print("\n" + "="*70)
    print("CuPy â†” Hologram Zero-Copy Exchange (GPU)")
    print("="*70)

    # CuPy â†’ Hologram (IMPORT from GPU)
    cupy_array = cp.random.randn(3, 4, dtype=cp.float32)
    print(f"\nCuPy array (GPU): shape={cupy_array.shape}")

    hologram_tensor = hg.HologramTensor.from_dlpack(cupy_array)
    print(f"\nâœ… Imported to Hologram: {hologram_tensor}")
    print(f"   Device: {hologram_tensor.device_info()}")

    # Hologram â†’ CuPy (EXPORT to GPU)
    cupy_array_2 = cp.from_dlpack(hologram_tensor)
    print(f"âœ… Exported back to CuPy: {cupy_array_2.shape}")

    print("âœ… Zero-copy confirmed: GPU arrays share memory")


if __name__ == "__main__":
    print("\n" + "="*70)
    print("Universal Zero-Copy Tensor Exchange via DLPack")
    print("="*70)
    print("\nDemonstrating Hologram's universal tensor interoperability")
    print("All exchanges are ZERO-COPY - no data duplication!")

    demo_pytorch()

    try:
        demo_jax()
    except ImportError:
        print("\nâš  JAX not installed, skipping JAX demo")

    try:
        demo_tensorflow()
    except ImportError:
        print("\nâš  TensorFlow not installed, skipping TensorFlow demo")

    try:
        demo_cupy()
    except ImportError:
        print("\nâš  CuPy not installed, skipping CuPy demo")

    print("\n" + "="*70)
    print("âœ… Universal DLPack Integration Complete!")
    print("="*70)
    print("\nKey Benefits:")
    print("  â€¢ Zero-copy tensor exchange with ALL frameworks")
    print("  â€¢ No framework-specific code needed")
    print("  â€¢ Works on CPU and GPU")
    print("  â€¢ 1000-10000Ã— faster than copying")
```

---

## Benefits of Universal DLPack Architecture

### 1. **Single Implementation, Universal Support**

- Write DLPack once, works with ALL frameworks
- No need for PyTorch-specific, JAX-specific, TF-specific code
- Future frameworks get support automatically

### 2. **True Zero-Copy Everywhere**

- No memory duplication
- Direct pointer sharing between frameworks
- Works on CPU and GPU

### 3. **Bidirectional Sharing**

```python
# Import FROM any framework
hologram_tensor = HologramTensor.from_dlpack(pytorch_tensor)
hologram_tensor = HologramTensor.from_dlpack(jax_array)
hologram_tensor = HologramTensor.from_dlpack(tf_tensor)

# Export TO any framework
pytorch_tensor = torch.from_dlpack(hologram_tensor)
jax_array = jnp.from_dlpack(hologram_tensor)
tf_tensor = tf.experimental.dlpack.from_dlpack(hologram_tensor)
```

### 4. **Composability**

```python
# PyTorch â†’ Hologram â†’ JAX (all zero-copy!)
pytorch_tensor = torch.randn(100, 100)
hologram_tensor = HologramTensor.from_dlpack(pytorch_tensor)

# Run Hologram operation
hg.vector_add_f32(exec, hologram_tensor, hologram_tensor, result, 10000)

# Export result to JAX
jax_array = jnp.from_dlpack(result)
```

---

## Comparison: DLPack vs Custom Integrations

### âŒ Custom Approach (Not Recommended)

```
Hologram â†’ PyTorch: Custom PyTorch extension (2-3 weeks)
Hologram â†’ JAX: Custom JAX primitive (2-3 weeks)
Hologram â†’ TensorFlow: Custom TF op (2-3 weeks)
Hologram â†’ CuPy: Custom CuPy wrapper (1 week)
```

**Total**: 8-10 weeks + maintenance burden for each framework

### âœ… Universal DLPack Approach (Recommended)

```
Hologram â†’ DLPack: Single implementation (1 week)
  â†“
Works with: PyTorch, JAX, TensorFlow, CuPy, PaddlePaddle, MXNet, ...
```

**Total**: 1 week + automatic support for all current and future frameworks

---

## Implementation Checklist

### Phase 2A: Import Support (High Priority)

- [ ] **Rust Core** (`hologram-core/src/executor.rs`)
  - [ ] Implement `tensor_from_dlpack()` for CPU
  - [ ] Implement `tensor_from_dlpack()` for CUDA
  - [ ] Add buffer view support (external memory)
  - [ ] Add external tensor registry with cleanup

- [ ] **FFI Layer** (`hologram-ffi/src/tensor.rs`)
  - [ ] Add `tensor_from_dlpack_capsule()` FFI function
  - [ ] Add to UDL interface
  - [ ] Export via lib.rs

- [ ] **Python Wrapper** (`hologram_ffi/tensor.py`)
  - [ ] Implement `HologramTensor.from_dlpack()` classmethod
  - [ ] Add external reference tracking
  - [ ] Update `__init__.py` exports

- [ ] **Testing**
  - [ ] Unit tests for `tensor_from_dlpack()`
  - [ ] Integration tests with PyTorch
  - [ ] Round-trip tests (PyTorch â†’ Hologram â†’ PyTorch)
  - [ ] Memory safety tests

### Phase 2B: Universal Framework Support (Medium Priority)

- [ ] **JAX Integration**
  - [ ] Test import from JAX arrays
  - [ ] Test export to JAX arrays
  - [ ] Add JAX examples

- [ ] **TensorFlow Integration**
  - [ ] Test import from TF tensors
  - [ ] Test export to TF tensors
  - [ ] Add TensorFlow examples

- [ ] **CuPy Integration** (GPU arrays)
  - [ ] Test import from CuPy arrays
  - [ ] Test export to CuPy arrays
  - [ ] Add CuPy GPU examples

- [ ] **Documentation**
  - [ ] Update README with universal support
  - [ ] Create framework comparison guide
  - [ ] Add best practices guide

### Phase 2C: Advanced Features (Low Priority)

- [ ] **Async Support**
  - [ ] CUDA stream synchronization
  - [ ] Async tensor transfer

- [ ] **Memory Management**
  - [ ] Smart reference counting
  - [ ] Automatic cleanup of external tensors

- [ ] **Performance**
  - [ ] Benchmark vs copying
  - [ ] Optimize for large tensors

---

## Success Criteria

âœ… **Phase 2A Complete When:**
- Import from PyTorch works (zero-copy)
- Round-trip PyTorch â†’ Hologram â†’ PyTorch works
- All tests pass
- Memory safety verified (no leaks, no crashes)

âœ… **Phase 2B Complete When:**
- Import/export works with PyTorch, JAX, TensorFlow
- Universal examples demonstrate all frameworks
- Documentation covers all frameworks

âœ… **Full Universal DLPack Complete When:**
- Single API works with ALL frameworks
- Zero-copy confirmed for all paths
- Performance benchmarks show 1000Ã— speedup
- Production-ready with comprehensive tests

---

## Conclusion

**DLPack IS the universal zero-copy architecture we should use.**

Instead of building custom integrations for each framework, we leverage DLPack's standard protocol to get universal support with a single implementation.

**Next Steps**:
1. Implement `tensor_from_dlpack()` in Rust
2. Add Python `from_dlpack()` classmethod
3. Test with PyTorch (our reference framework)
4. Extend to JAX, TensorFlow, CuPy
5. Document universal architecture

**Timeline**: 1-2 weeks for complete universal DLPack support

**ROI**: Instead of 8-10 weeks for custom integrations, we get universal support in 1-2 weeks + automatic future framework compatibility.

---

**Document Version**: 1.0
**Status**: Proposed - Ready for Implementation
**Next**: Implement Phase 2A (Import Support)
