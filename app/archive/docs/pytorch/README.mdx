---
title: "PyTorch Integration for Hologram"
description: "PyTorch Integration for Hologram documentation"
---

# PyTorch Integration for Hologram

This directory contains documentation for integrating Hologram with PyTorch for high-performance ML workloads.

## Quick Links

- **[Implementation Plan](implementation-plan.md)** - Complete roadmap for PyTorch integration
- **[Implementation Status](implementation-status.md)** - Detailed progress report
- **[Recommendations](RECOMMENDATIONS.md)** - Practical next steps and quick wins

## Summary

### What's Complete ‚úÖ

**Phase 1: CUDA Backend (Production-Ready)**
- PTX compilation system with build-time kernel compilation
- Lazy module loading with kernel caching (O(1) lookup)
- Full GPU execution for 21 element-wise operations
- Graceful CPU fallback if CUDA unavailable

**Expected Performance**: 10-50√ó speedup on NVIDIA GPUs for operations > 1M elements

### What's In Progress üü°

**Phase 2: DLPack Integration (70% Complete)**
- DLPack C ABI bindings implemented
- Standard-compliant structs for zero-copy tensor exchange
- Foundation ready for PyTorch/JAX/TensorFlow integration

**Remaining**: 9-14 hours to complete executor integration and FFI exposure

### What's Next ‚è≥

- **Phase 3**: Benchmarking suite (1-2 days)
- **Phase 4**: CUDA stream sharing (2-3 days)
- **Phase 5**: Autograd integration (3-5 days)
- **Phase 6**: Custom PyTorch operators (2-3 days)
- **Phase 7**: Multi-GPU support (3-5 days)

## Quick Start

### Enable GPU Acceleration

If you have CUDA Toolkit installed:

```bash
# Build with CUDA support
cargo build -p hologram-backends --features cuda --release
cargo build -p hologram-ffi --features cuda --release

# Update Python bindings
cd crates/hologram-ffi
./scripts/update_ffi_bindings.sh
```

### Run PyTorch Example

```bash
# Ensure PyTorch is installed
pip install torch numpy

# Run integration example
python examples/pytorch_hologram_integration.py
```

**Note**: Current example uses JSON serialization (slow). See recommendations for improvements.

## Performance

### Current Performance Issues

The PyTorch example has known performance issues:

1. **JSON Serialization** (1000-10000√ó overhead):
   - Lines 100-122: `json.dumps()` for tensor transfer
   - Lines 137-138: `json.loads()` for results

2. **No GPU Execution** (requires CUDA build)

3. **No Benchmarking** (can't measure improvements)

### Quick Wins (< 2 hours)

1. **Remove JSON**: Use `buffer_copy_from_bytes()` (already in FFI)
2. **Add Benchmarks**: Compare Hologram vs PyTorch baseline
3. **Enable CUDA**: Build with `--features cuda`

**Expected Result**: 100-1000√ó improvement over current example

### With DLPack Complete

- Zero memory copies (true zero-copy tensors)
- 2√ó memory reduction (no duplicate storage)
- Streamlined API for tensor sharing

## Architecture

```
Application Code (PyTorch)
    ‚Üì
hologram-ffi (Python bindings)
    ‚Üì
hologram-core (Operations)
    ‚Üì
hologram-backends (CPU/GPU execution)
    ‚Üì
CUDA Kernels (GPU) or SIMD (CPU)
```

**Key Optimizations**:
- Build-time PTX compilation (zero runtime overhead)
- Kernel caching (one-time load cost)
- Pattern-based GPU dispatch
- CPU SIMD fallback (881-4,367√ó vs scalar)

## Files in This Directory

| File | Description | Lines |
|------|-------------|-------|
| `README.md` | This file | - |
| `implementation-plan.md` | Complete roadmap | 256 |
| `implementation-status.md` | Detailed progress report | 490 |
| `RECOMMENDATIONS.md` | Practical next steps | 280 |

## Implementation Files

### Created Files

**CUDA Backend**:
- `crates/hologram-backends/build.rs` - PTX compilation (162 lines)

**DLPack Integration**:
- `crates/hologram-core/src/interop/dlpack.rs` - C ABI bindings (277 lines)
- `crates/hologram-core/src/interop/mod.rs` - Module organization

### Modified Files

**CUDA Backend**:
- `crates/hologram-backends/src/backends/cuda/mod.rs` - Kernel loading/launch (+150 lines)
- `crates/hologram-backends/src/backends/cuda/memory.rs` - Device pointer access

**DLPack Integration**:
- `crates/hologram-core/src/lib.rs` - Exposed interop module
- `crates/hologram-core/src/tensor.rs` - Added DLPack methods

## Testing

### Build Verification

```bash
# Test CPU build (no CUDA)
cargo build -p hologram-backends

# Test with CUDA (requires nvcc)
cargo build -p hologram-backends --features cuda

# Run all tests
cargo test --workspace
```

### GPU Testing

Requires:
- NVIDIA GPU
- CUDA Toolkit 11.0+
- nvcc compiler

```bash
# Build with CUDA
cargo build -p hologram-backends --features cuda --release

# Run CUDA tests
cargo test -p hologram-backends --features cuda backends::cuda
```

## Benchmarking

### Current Benchmarks

Existing hologram-core benchmarks:
- `benches/math_ops.rs` - CPU performance
- `benches/inline_performance.rs` - SIMD fast paths
- `benches/native_vs_hologram_bench.rs` - Hologram vs native comparison

### Planned PyTorch Benchmarks

1. **Element-wise Operations**:
   - Sizes: 1K, 10K, 100K, 1M, 10M elements
   - Operations: add, sub, mul, div
   - Metrics: Latency (¬µs), Throughput (GB/s)

2. **DLPack Overhead**:
   - Copy vs zero-copy comparison
   - Tensor creation/destruction overhead
   - Memory usage comparison

3. **End-to-End Pipelines**:
   - PyTorch-only baseline
   - PyTorch + Hologram hybrid
   - Full Hologram (when complete)

## Troubleshooting

### Build Errors

**Error: `nvcc not found`**
- Install CUDA Toolkit
- Add nvcc to PATH
- Or build without `cuda` feature

**Error: `CUDA kernels not compiled`**
- Check build output for PTX compilation warnings
- Verify nvcc is accessible
- Check `target/debug/build/hologram-backends-*/out/atlas_kernels.ptx`

### Runtime Errors

**Error: `CUDA device not found`**
- Verify NVIDIA GPU present
- Check CUDA drivers installed
- Build fell back to CPU (expected behavior)

**Error: `Pattern matching failed`**
- Operation not yet supported on GPU
- Falls back to CPU execution
- Check supported operations in `cuda/mod.rs`

## Contributing

### Adding New CUDA Kernels

1. Add kernel to `src/backends/cuda/kernels.cu`
2. Add kernel name to `ensure_kernels_loaded()`
3. Update pattern recognition in `try_recognize_*`
4. Test with `cargo test --features cuda`

### Completing DLPack

1. Implement `Executor::tensor_to_dlpack()` in `executor.rs`
2. Add deleter function for proper cleanup
3. Expose in `hologram-ffi/src/hologram_ffi.udl`
4. Create PyCapsule wrappers for Python
5. Update PyTorch example

See `RECOMMENDATIONS.md` for detailed steps.

## Resources

### External Documentation

- [DLPack Specification](https://dmlc.github.io/dlpack/latest/)
- [PyTorch C++ Extension Guide](https://pytorch.org/tutorials/advanced/cpp_extension.html)
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

### Internal Documentation

- [Hologram Backend Architecture](../BACKEND_ARCHITECTURE.md)
- [CPU Backend Tracing](../CPU_BACKEND_TRACING.md)

## Support

For questions or issues:
1. Check documentation in this directory
2. Review example code in `examples/pytorch_hologram_integration.py`
3. File issues on GitHub repository

## License

See repository LICENSE file.
